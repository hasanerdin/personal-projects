{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DP methods, we found optimal policies. But, we used ```transition_map``` to do that. This is quite impractical. There are many reasons for that:\n",
    "\n",
    "- We don't know all the states we can visit\n",
    "- Environment dynamics can be unknown\n",
    "- Iterating over all states can be computationally infeasible\n",
    "\n",
    "In this notebook, we will be practicing over TD methods. Instead of calculating the exact value, we will get samples and estimate the value over samples. One approach that comes into our minds, when we talk about sampling, is **Monte Carlo** method. MC method is the first algorithm we will be implementing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "\n",
    "We will be using a new environment called **Warehouse** where an agent tries to match each item with the corresponding box. Building a transition map for this environment would be painful. Luckily, we don't need to build that in TD methods. The ```worldmap``` is given below (you can modify this for fun but, we will be using the one below to evaluate).\n",
    "\n",
    "You can run the cell below to visualize the environment.\n",
    "- <span style=\"color:#989898\">Dark gray cells</span> are impassable cells.\n",
    "- <span style=\"color:#DADADA\">light gray cells</span> are passable empty cells.\n",
    "- <span style=\"color:#00B8FA\">Blue cell</span> is the agent.\n",
    "- <span style=\"color:#A33675\"> Darker magenta cell </span> and <span style=\"color:#48C69F\"> darker green cell </span> are items to collect. \n",
    "- <span style=\"color:#B34685\"> Lighter magenta cell </span> and <span style=\"color:#58D6AF\"> lighter green cell </span> are the boxes.\n",
    "\n",
    "Pairing is also given below. A key in the pairing dictionary is a box(uppercase) and its corresponding value is a list of items(lowercase) that can be delivered to that box.\n",
    "\n",
    "State representation is different here. Instead of giving just the position of the agent, we have 4 more additional features. These are 4 boolean values for two items and two boxes.  The boolean feature is true if the item or the box exists on the map and false otherwise. When the bucket receives the item it disappears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from env import Warehouse\n",
    "\n",
    "worldmap = [\"#########\",\n",
    "            \"#   #   #\",\n",
    "            \"# c # C #\",\n",
    "            \"#   #   #\",\n",
    "            \"#   P   #\",\n",
    "            \"#   #   #\",\n",
    "            \"# b # B #\",\n",
    "            \"#   #   #\",\n",
    "            \"#########\"]\n",
    "\n",
    "# Buckets are Uppercase letters while balls are lowercase\n",
    "# Matching is done so that the ball \"b\" must be carried to the bucket \"B\"\n",
    "pairing = {\n",
    "    \"B\": [\"b\"],\n",
    "    \"C\": [\"c\"]\n",
    "}\n",
    "\n",
    "env = Warehouse(balls=\"cb\", buckets=\"BC\",\n",
    "                pairing=pairing, worldmap=worldmap)\n",
    "env.init_render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mc_agent import MCAgent\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize agent\n",
    "agent = MCAgent(n_acts=4)\n",
    "\n",
    "# Hyperparameters\n",
    "args = dict(\n",
    "    iteration = 800,\n",
    "    gamma = 0.99,\n",
    "    alpha = 0.03,\n",
    "    init_eps = 0.9,\n",
    "    final_eps = 0.1,\n",
    "    eps_decay_rate = 0.995,\n",
    "    seed = 12021,            # Current year: (10000 + 2021 = 12021) Holocene calendar\n",
    ")\n",
    "args = namedtuple('args', args.keys())(*args.values()) \n",
    "\n",
    "# Seed\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "# Traning loop\n",
    "reward_list = []\n",
    "epsilon = args.init_eps\n",
    "for ix in range(args.iteration):\n",
    "    epsilon = max(args.final_eps, epsilon*args.eps_decay_rate)\n",
    "    reward = agent.one_episode_train(\n",
    "        env, lambda x: agent.e_greedy_policy(x, epsilon), args.gamma)\n",
    "    reward_list.append(reward)\n",
    "    if ((ix + 1) % 50) == 0:\n",
    "        print(\"Episode: {}, reward: {}\".format(ix + 1, np.mean(reward_list[-100:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to render the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Warehouse(balls=\"cb\", buckets=\"BC\",\n",
    "                pairing=pairing, worldmap=worldmap)\n",
    "\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.evaluate(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC method waits until the episode is terminated to begin the update. But, it is possible to update the policy within every transition. Temporal Difference(TD) methods exactly aim for this. There are two popular TD methods that you will be implementing: Q Learning and SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q - Learning\n",
    "\n",
    "Q learning is an off-policy algorithm that employs temporal difference value estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate environment\n",
    "env = Warehouse(balls=\"cb\", buckets=\"BC\",\n",
    "                pairing=pairing, worldmap=worldmap)\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from td_agents import QAgent\n",
    "\n",
    "# Initialize agent\n",
    "q_agent = QAgent(n_acts=4)\n",
    "# Hyperparameters\n",
    "args = dict(\n",
    "    episodes = 600,\n",
    "    evaluate_period = 50,\n",
    "    gamma = 0.9,\n",
    "    alpha = 0.4,\n",
    "    init_eps = 0.9,\n",
    "    final_eps = 0.1,\n",
    "    eps_decay_rate = 0.999,\n",
    "    seed = 12021,            # Current year: (10000 + 2021 = 12021) Holocene calendar\n",
    ")\n",
    "args = namedtuple('args', args.keys())(*args.values()) \n",
    "\n",
    "# Seed\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "\n",
    "q_rewards = q_agent.train(env, q_agent.e_greedy_policy, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to render the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Warehouse(balls=\"cb\", buckets=\"BC\",\n",
    "                pairing=pairing, worldmap=worldmap)\n",
    "\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.evaluate(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate environment\n",
    "env = Warehouse(balls=\"cb\", buckets=\"BC\",\n",
    "                pairing=pairing, worldmap=worldmap)\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from td_agents import SarsaAgent\n",
    "\n",
    "# Initialize agent\n",
    "s_agent = SarsaAgent(n_acts=4)\n",
    "# Hyperparameters\n",
    "args = dict(\n",
    "    episodes = 800,\n",
    "    evaluate_period = 50,\n",
    "    gamma = 0.9,\n",
    "    alpha = 0.5,\n",
    "    init_eps = 0.9,\n",
    "    final_eps = 0.1,\n",
    "    eps_decay_rate = 0.995,\n",
    "    seed = 12021,            # Current year: (10000 + 2021 = 12021) Holocene calendar\n",
    ")\n",
    "args = namedtuple('args', args.keys())(*args.values()) \n",
    "\n",
    "# Seed\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "\n",
    "s_rewards = s_agent.train(env, s_agent.e_greedy_policy, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to render the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Warehouse(balls=\"cb\", buckets=\"BC\",\n",
    "                pairing=pairing, worldmap=worldmap)\n",
    "\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_agent.evaluate(env, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
